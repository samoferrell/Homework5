[
  {
    "objectID": "Homework5quarto.html",
    "href": "Homework5quarto.html",
    "title": "Homework 5 - ST558",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model?\n\nWith random forest models, there are a lot of changes we can make to the model, using cross validation can help determine better shrinkage, n.trees, and interaction.depth. Also, if we have time, we can fit many models to compare with cross validation. This informs us if we should use a simpler/easier to implement model of the performance drop isn’t large.\n\nDescribe the bagged tree algorithm.\n\nThe bagged tree algorithm for is as follows:\n\n\n\nCreate a bootstrap sample (same size as actual sample)\n\n\n\n\nTrain tree on this sample (no pruning necessary)\n\n\n\n\nRepeat B = 1000 times\n\n\n\n\n(Regression) Final prediction is average of these prediction\n\n\n\n\n(Classification) Final prediction can be majority vote (most common prediction made by all bootstrap trees)\n\n\nWhat is meant by a general linear model?\n\nA general linear model is a model that is for continuous responses, but allows for both continuous and categorical predictors.\n\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\nAdding an interaction term helps capture any dependent relationship/effect between predictors. In a model without the interaction term, all predictors are assumed to have an independnt effect on the response.\n\nWhy do we split our data into a training and test set?\n\nWe split our data into a training set to train the model and then with the test set of the data we can see how accurate the model is at predicting the correct variable. By doing this we are ensuring no data leakage and creating a solid unbiased model."
  },
  {
    "objectID": "Homework5quarto.html#task-1-conceptual-questions",
    "href": "Homework5quarto.html#task-1-conceptual-questions",
    "title": "Homework 5 - ST558",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model?\n\nWith random forest models, there are a lot of changes we can make to the model, using cross validation can help determine better shrinkage, n.trees, and interaction.depth. Also, if we have time, we can fit many models to compare with cross validation. This informs us if we should use a simpler/easier to implement model of the performance drop isn’t large.\n\nDescribe the bagged tree algorithm.\n\nThe bagged tree algorithm for is as follows:\n\n\n\nCreate a bootstrap sample (same size as actual sample)\n\n\n\n\nTrain tree on this sample (no pruning necessary)\n\n\n\n\nRepeat B = 1000 times\n\n\n\n\n(Regression) Final prediction is average of these prediction\n\n\n\n\n(Classification) Final prediction can be majority vote (most common prediction made by all bootstrap trees)\n\n\nWhat is meant by a general linear model?\n\nA general linear model is a model that is for continuous responses, but allows for both continuous and categorical predictors.\n\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\nAdding an interaction term helps capture any dependent relationship/effect between predictors. In a model without the interaction term, all predictors are assumed to have an independnt effect on the response.\n\nWhy do we split our data into a training and test set?\n\nWe split our data into a training set to train the model and then with the test set of the data we can see how accurate the model is at predicting the correct variable. By doing this we are ensuring no data leakage and creating a solid unbiased model."
  },
  {
    "objectID": "Homework5quarto.html#task-2-fitting-models",
    "href": "Homework5quarto.html#task-2-fitting-models",
    "title": "Homework 5 - ST558",
    "section": "Task 2: Fitting Models",
    "text": "Task 2: Fitting Models\nThese are the libraries accessed for the assignment.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nNow, we will be reading in the data. I have saved it into the folder for my repo.\n\nheart &lt;- read_csv(\"heart.csv\")\n\nRows: 918 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope\ndbl (7): Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak, HeartDisease\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNow we will quickly understand your data. We will check on missingness and summarize the data, especially with respect to the relationships of the variables to HeartDisease\n\nheartNA &lt;- heart |&gt;\n  filter(if_any(everything(), is.na))\n# no missing data\n\n# Generating tables of the variables\ntable(heart$ChestPainType, heart$HeartDisease)\n\n     \n        0   1\n  ASY 104 392\n  ATA 149  24\n  NAP 131  72\n  TA   26  20\n\ntable(heart$Sex, heart$HeartDisease)\n\n   \n      0   1\n  F 143  50\n  M 267 458\n\ntable(heart$RestingECG, heart$HeartDisease)\n\n        \n           0   1\n  LVH     82 106\n  Normal 267 285\n  ST      61 117\n\n\nNumerical Summaries of our numerical variables:\n\nsummary(heart |&gt; select(where(is.numeric)))\n\n      Age          RestingBP      Cholesterol      FastingBS     \n Min.   :28.00   Min.   :  0.0   Min.   :  0.0   Min.   :0.0000  \n 1st Qu.:47.00   1st Qu.:120.0   1st Qu.:173.2   1st Qu.:0.0000  \n Median :54.00   Median :130.0   Median :223.0   Median :0.0000  \n Mean   :53.51   Mean   :132.4   Mean   :198.8   Mean   :0.2331  \n 3rd Qu.:60.00   3rd Qu.:140.0   3rd Qu.:267.0   3rd Qu.:0.0000  \n Max.   :77.00   Max.   :200.0   Max.   :603.0   Max.   :1.0000  \n     MaxHR          Oldpeak         HeartDisease   \n Min.   : 60.0   Min.   :-2.6000   Min.   :0.0000  \n 1st Qu.:120.0   1st Qu.: 0.0000   1st Qu.:0.0000  \n Median :138.0   Median : 0.6000   Median :1.0000  \n Mean   :136.8   Mean   : 0.8874   Mean   :0.5534  \n 3rd Qu.:156.0   3rd Qu.: 1.5000   3rd Qu.:1.0000  \n Max.   :202.0   Max.   : 6.2000   Max.   :1.0000  \n\n\nNow we will create a new variable that is a factor version of the HeartDisease variable. We will also remove the ST_Slope variable and the original HeartDisease variable.\n\nheart_new &lt;- heart |&gt;\n  mutate(HeartDiseaseF = as.factor(HeartDisease)) |&gt; # changing to factor\n  select(-c(ST_Slope,HeartDisease)) # removing ST_slope and original HeartDisease variable\n\nLooking at our numeric variables, we can see there isn’t a super high correlation that sticks out that would indicate collinearity.\n\ncor(heart_new |&gt; select(where(is.numeric)))\n\n                    Age   RestingBP Cholesterol   FastingBS      MaxHR\nAge          1.00000000  0.25439936 -0.09528177  0.19803907 -0.3820447\nRestingBP    0.25439936  1.00000000  0.10089294  0.07019334 -0.1121350\nCholesterol -0.09528177  0.10089294  1.00000000 -0.26097433  0.2357924\nFastingBS    0.19803907  0.07019334 -0.26097433  1.00000000 -0.1314385\nMaxHR       -0.38204468 -0.11213500  0.23579240 -0.13143849  1.0000000\nOldpeak      0.25861154  0.16480304  0.05014811  0.05269786 -0.1606906\n                Oldpeak\nAge          0.25861154\nRestingBP    0.16480304\nCholesterol  0.05014811\nFastingBS    0.05269786\nMaxHR       -0.16069055\nOldpeak      1.00000000\n\n\nNow we will create dummy columns corresponding to the values of these variables (Sex, ExerciseAngina, ChestPainType, and RestingECG) for use in our kNN fit.\n\n# generating a dummy variable model\nmodel &lt;- dummyVars(~ Sex + ExerciseAngina + ChestPainType + RestingECG, data = heart_new)\n# applying it on the data\nheart_dummy &lt;- as.data.frame(predict(model, newdata = heart_new))\n# combining with our original data\nheart_new_dummy &lt;- cbind(heart_new, heart_dummy)\n\nNow we will split our data into a training and test set.\n\nset.seed(3)\n# Creating an 80/20 split\nsplit &lt;- createDataPartition(y = heart_new_dummy$HeartDiseaseF, p = 0.8, list = FALSE)\ntrain &lt;- heart_new_dummy[split, ]\ntest &lt;- heart_new_dummy[-split, ]\ndim(train)\n\n[1] 735  22\n\n\n\nkNN\nNext, we’ll fit a kNN model. We will use repeated 10 fold cross-validation, with the number of repeats being 3. We will also preprocess the data by centering and scaling.\n\nModel\n\n# removing non-numeric variables for easier modeling\ntrain_numeric &lt;- train |&gt;\n  select(-c(Sex, ChestPainType, RestingECG, ExerciseAngina))\n# centering and scaling\npreProcValues &lt;- preProcess(train_numeric, method = c(\"center\", \"scale\"))\ntrainTransformed &lt;- predict(preProcValues, train_numeric)\n\n# knn model\nfit1 &lt;- train(HeartDiseaseF ~ ., \n              data = trainTransformed,\n              method = \"knn\",\n              trControl = trainControl(method = \"repeatedcv\", \n                                       number = 10,\n                                       repeats = 3,\n              ),\n              tuneGrid = expand.grid(k = seq(from = 1, to = 40, by = 1))\n)\n\n\n\nPlot\n\nplot(fit1)\n\n\n\n\n\n\n\n\n\n\nAnalysis\nNow, we will dislay our confusion matrix to see how well the model performed.\n\n# pre-procesing my test data prior to running the model:\ntest_numeric &lt;- test |&gt;\n  select(-c(Sex, ChestPainType, RestingECG, ExerciseAngina))\npreProcValues &lt;- preProcess(test_numeric, method = c(\"center\", \"scale\"))\ntestTransformed &lt;- predict(preProcValues, test_numeric)\n\n# confusion matrix\nconfusionMatrix(data = testTransformed$HeartDiseaseF, \n                reference = predict(fit1, newdata = testTransformed))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 64 18\n         1 21 80\n                                          \n               Accuracy : 0.7869          \n                 95% CI : (0.7204, 0.8438)\n    No Information Rate : 0.5355          \n    P-Value [Acc &gt; NIR] : 1.39e-12        \n                                          \n                  Kappa : 0.5706          \n                                          \n Mcnemar's Test P-Value : 0.7488          \n                                          \n            Sensitivity : 0.7529          \n            Specificity : 0.8163          \n         Pos Pred Value : 0.7805          \n         Neg Pred Value : 0.7921          \n             Prevalence : 0.4645          \n         Detection Rate : 0.3497          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.7846          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\n\nLogistic Regression\nUsing our EDA, we will posit three different logistic regression models.\n\n# three models\nmodel1 &lt;- HeartDiseaseF ~ Age + Sex + Cholesterol\nmodel2 &lt;- HeartDiseaseF ~ RestingBP + FastingBS + MaxHR\nmodel3 &lt;- HeartDiseaseF ~ ChestPainType*RestingECG\n\n\nglmFit1 &lt;- train(model1, \n              data = train,\n              method = \"glm\",\n              family = \"binomial\",\n              trControl = trainControl(method = \"repeatedcv\", \n                                       number = 10,\n                                       repeats = 3))\nglmFit2 &lt;- train(model2, \n              data = train,\n              method = \"glm\",\n              family = \"binomial\",\n              trControl = trainControl(method = \"repeatedcv\", \n                                       number = 10,\n                                       repeats = 3))\nglmFit3 &lt;- train(model3, \n              data = train,\n              method = \"glm\",\n              family = \"binomial\",\n              trControl = trainControl(method = \"repeatedcv\", \n                                       number = 10,\n                                       repeats = 3))\n\n\nAnalysis\n\ncat(\"Model 1 Accuracy: \",\nglmFit1$results$Accuracy)\n\nModel 1 Accuracy:  0.6902174\n\ncat(\"Model 2 Accuracy: \",\nglmFit2$results$Accuracy)\n\nModel 2 Accuracy:  0.7056759\n\ncat(\"Model 3 Accuracy: \",\nglmFit3$results$Accuracy)\n\nModel 3 Accuracy:  0.7573831\n\n\nAs we can see our 3rd model was the most accurate. Let’s look at a basic summary of it:\n\nsummary(glmFit3)\n\n\nCall:\nNULL\n\nCoefficients:\n                                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                          1.4008932  0.2707697   5.174 2.29e-07 ***\nChestPainTypeATA                    -2.0070290  0.5752321  -3.489 0.000485 ***\nChestPainTypeNAP                    -2.5358731  0.4691923  -5.405 6.49e-08 ***\nChestPainTypeTA                     -2.0940403  0.6109960  -3.427 0.000610 ***\nRestingECGNormal                    -0.0977492  0.3168172  -0.309 0.757675    \nRestingECGST                         0.0009054  0.3888455   0.002 0.998142    \n`ChestPainTypeATA:RestingECGNormal` -1.6097498  0.6928163  -2.323 0.020153 *  \n`ChestPainTypeNAP:RestingECGNormal`  0.5105944  0.5393475   0.947 0.343797    \n`ChestPainTypeTA:RestingECGNormal`   0.7908963  0.7748374   1.021 0.307385    \n`ChestPainTypeATA:RestingECGST`     -0.3755988  0.7987133  -0.470 0.638173    \n`ChestPainTypeNAP:RestingECGST`      1.7759284  0.6713103   2.645 0.008158 ** \n`ChestPainTypeTA:RestingECGST`       0.6922418  1.2046579   0.575 0.565536    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1010.42  on 734  degrees of freedom\nResidual deviance:  764.86  on 723  degrees of freedom\nAIC: 788.86\n\nNumber of Fisher Scoring iterations: 4\n\n\nNow we will predict with the test data and model 3 and see how we do:\n\nconfusionMatrix(data = test$HeartDiseaseF, \n                reference = predict(glmFit3, newdata = test))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 55 27\n         1 15 86\n                                          \n               Accuracy : 0.7705          \n                 95% CI : (0.7027, 0.8293)\n    No Information Rate : 0.6175          \n    P-Value [Acc &gt; NIR] : 7.564e-06       \n                                          \n                  Kappa : 0.5295          \n                                          \n Mcnemar's Test P-Value : 0.08963         \n                                          \n            Sensitivity : 0.7857          \n            Specificity : 0.7611          \n         Pos Pred Value : 0.6707          \n         Neg Pred Value : 0.8515          \n             Prevalence : 0.3825          \n         Detection Rate : 0.3005          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.7734          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\n\nTree Models\nIn this section we’ll fit a few different tree based models in a similar way as above! We will be using repeated 10 fold CV to select a best model for Tree method comparison.\nThis is the model we will be using:\n\nmodel &lt;- HeartDiseaseF ~ Sex + Age + ExerciseAngina + Oldpeak + ChestPainType\n\n\nClassification Tree Model\nWe will start with a classification tree model:\n\ntreeFit &lt;- train(model, \n              data = train,\n              method = \"rpart\",\n              trControl = trainControl(method = \"repeatedcv\", \n                                       number = 10,\n                                       repeats = 3),\n              tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.001))\n              )\n\n\nPlot\nThis is a plot of our model:\n\nplot(treeFit)\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest\nNow we will create a Random Forest model:\n\nrfFit &lt;- train(model, \n              data = train,\n              method = \"rf\",\n              trControl = trainControl(method = \"repeatedcv\", \n                                       number = 10,\n                                       repeats = 3),\n              tuneGrid = expand.grid(mtry = seq(from = 1, to = length(model1), by = 1))\n              )\n\n\n\nBoosted Tree\nNow we will generate a Boosted Tree model:\n\nboostedFit &lt;- train(model, \n              data = train,\n              method = \"gbm\",\n              trControl = trainControl(method = \"repeatedcv\", \n                                       number = 10,\n                                       repeats = 3),\n              tuneGrid = expand.grid(n.trees = c(25,50,100,200),\n                                     interaction.depth = c(1,2,3),\n                                     shrinkage = 0.1,\n                                     n.minobsinnode = 10\n                                     ),\n              verbose = FALSE)\n\n\n\nComparison of the Three Models\nWe will generate the confusion matrix of all three models in order, and access the accuracy of each model:\n\ncat(\"Classification Tree Model\", \nconfusionMatrix(data = test$HeartDiseaseF,\n                reference = predict(treeFit, newdata = test))$overall[1])\n\nClassification Tree Model 0.7923497\n\ncat(\"Random Forest Model\",\nconfusionMatrix(data = test$HeartDiseaseF,\n                reference = predict(rfFit, newdata = test))$overall[1])\n\nRandom Forest Model 0.7978142\n\ncat(\"Boosted Tree Model\",\nconfusionMatrix(data = test$HeartDiseaseF,\n                reference = predict(boostedFit, newdata = test))$overall[1])\n\nBoosted Tree Model 0.8032787\n\n\n\n\n\nFinal Analysis\nAs we can see, we nearly got 80% accuracy with our Boosted Model, giving it the highest accuracy of all the models we went through. This model overall did the best job on the test set."
  }
]