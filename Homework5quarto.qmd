---
title: "Homework 5 - ST558"
author: "Samuel O'Ferrell"
description: "This homework shows practice fitting models and using the caret package. First, conceptual questions are discussed. Then, we will be performing some EDA on a heart disease data set. Once ready, we will be fitting several different types of models: kNN, Logistic Regression, Tree Models. We will be using repeated CV and displaying confusion matrices of the accuracy of the models on the test data."
format: html
editor: visual
---

## Task 1: Conceptual Questions

What is the purpose of using cross-validation when fitting a random forest model?

> With random forest models, there are a lot of changes we can make to the model, using cross validation can  help determine better shrinkage, n.trees, and interaction.depth. Also, if we have time, we can fit many models to compare with cross validation. This informs us if we should use a simpler/easier to implement model of the performance drop isn't large.

Describe the bagged tree algorithm.

> The bagged tree algorithm for is as follows:

> 1.  Create a bootstrap sample (same size as actual sample)

> 2.  Train tree on this sample (no pruning necessary)

> 3.  Repeat B = 1000 times

> 4.  (Regression) Final prediction is average of these prediction

> 4.  (Classification) Final prediction can be majority vote (most common prediction made by all bootstrap trees)

What is meant by a general linear model?

> A general linear model is a model that is for continuous responses, but allows for both continuous and categorical predictors.

When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?

> Adding an interaction term helps capture any dependent relationship/effect between predictors. In a model without the interaction term, all predictors are assumed to have an independnt effect on the response.

Why do we split our data into a training and test set?

> We split our data into a training set to train the model and then with the test set of the data we can see how accurate the model is at predicting the correct variable. By doing this we are ensuring no data leakage and creating a solid unbiased model.

## Task 2: Fitting Models

These are the libraries accessed for the assignment.
```{r}
library(tidyverse)
library(caret)
```
Now, we will be reading in the data. I have saved it into the folder for my repo.
```{r}
heart <- read_csv("heart.csv")
```

Now we will quickly understand your data. We will check on missingness and summarize the data, especially with respect to the relationships of the variables to HeartDisease

```{r}
heartNA <- heart |>
  filter(if_any(everything(), is.na))
# no missing data

# Generating tables of the variables
table(heart$ChestPainType, heart$HeartDisease)
table(heart$Sex, heart$HeartDisease)
table(heart$RestingECG, heart$HeartDisease)
```
Numerical Summaries of our numerical variables:
```{r}
summary(heart |> select(where(is.numeric)))
```

Now we will create a new variable that is a factor version of the HeartDisease variable. We will also remove the ST_Slope variable and the original HeartDisease variable.


```{r}
heart_new <- heart |>
  mutate(HeartDiseaseF = as.factor(HeartDisease)) |> # changing to factor
  select(-c(ST_Slope,HeartDisease)) # removing ST_slope and original HeartDisease variable
```

Looking at our numeric variables, we can see there isn't a super high correlation that sticks out that would indicate collinearity. 
```{r}
cor(heart_new |> select(where(is.numeric)))
```

Now we will create dummy columns corresponding to the values of these variables (Sex, ExerciseAngina, ChestPainType, and RestingECG) for use in our kNN fit. 

```{r}
# generating a dummy variable model
model <- dummyVars(~ Sex + ExerciseAngina + ChestPainType + RestingECG, data = heart_new)
# applying it on the data
heart_dummy <- as.data.frame(predict(model, newdata = heart_new))
# combining with our original data
heart_new_dummy <- cbind(heart_new, heart_dummy)
```

Now we will split our data into a training and test set.

```{r}
set.seed(3)
# Creating an 80/20 split
split <- createDataPartition(y = heart_new_dummy$HeartDiseaseF, p = 0.8, list = FALSE)
train <- heart_new_dummy[split, ]
test <- heart_new_dummy[-split, ]
dim(train)
```

### kNN

Next, we’ll fit a kNN model. We will use repeated 10 fold cross-validation, with the number of repeats being 3. We will also preprocess the data by centering and scaling. 

#### Model

```{r}
# removing non-numeric variables for easier modeling
train_numeric <- train |>
  select(-c(Sex, ChestPainType, RestingECG, ExerciseAngina))
# centering and scaling
preProcValues <- preProcess(train_numeric, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, train_numeric)

# knn model
fit1 <- train(HeartDiseaseF ~ ., 
              data = trainTransformed,
              method = "knn",
              trControl = trainControl(method = "repeatedcv", 
                                       number = 10,
                                       repeats = 3,
              ),
              tuneGrid = expand.grid(k = seq(from = 1, to = 40, by = 1))
)
```

#### Plot

```{r}
plot(fit1)
```

#### Analysis


Now, we will dislay our confusion matrix to see how well the model performed.
```{r}
# pre-procesing my test data prior to running the model:
test_numeric <- test |>
  select(-c(Sex, ChestPainType, RestingECG, ExerciseAngina))
preProcValues <- preProcess(test_numeric, method = c("center", "scale"))
testTransformed <- predict(preProcValues, test_numeric)

# confusion matrix
confusionMatrix(data = testTransformed$HeartDiseaseF, 
                reference = predict(fit1, newdata = testTransformed))
```

### Logistic Regression

Using our EDA, we will posit three different logistic regression models. 

```{r}
# three models
model1 <- HeartDiseaseF ~ Age + Sex + Cholesterol
model2 <- HeartDiseaseF ~ RestingBP + FastingBS + MaxHR
model3 <- HeartDiseaseF ~ ChestPainType*RestingECG
```


```{r}

glmFit1 <- train(model1, 
              data = train,
              method = "glm",
              family = "binomial",
              trControl = trainControl(method = "repeatedcv", 
                                       number = 10,
                                       repeats = 3))
glmFit2 <- train(model2, 
              data = train,
              method = "glm",
              family = "binomial",
              trControl = trainControl(method = "repeatedcv", 
                                       number = 10,
                                       repeats = 3))
glmFit3 <- train(model3, 
              data = train,
              method = "glm",
              family = "binomial",
              trControl = trainControl(method = "repeatedcv", 
                                       number = 10,
                                       repeats = 3))
```

#### Analysis
```{r}
cat("Model 1 Accuracy: ",
glmFit1$results$Accuracy)

cat("Model 2 Accuracy: ",
glmFit2$results$Accuracy)

cat("Model 3 Accuracy: ",
glmFit3$results$Accuracy)
```

As we can see our 3rd model was the most accurate. Let's look at a basic summary of it:

```{r}
summary(glmFit3)
```

Now we will predict with the test data and model 3 and see how we do:

```{r}
confusionMatrix(data = test$HeartDiseaseF, 
                reference = predict(glmFit3, newdata = test))
```

### Tree Models

In this section we’ll fit a few different tree based models in a similar way as above! We will be using repeated 10 fold CV to select a best model for Tree method comparison.

This is the model we will be using:
```{r}
model <- HeartDiseaseF ~ Sex + Age + ExerciseAngina + Oldpeak + ChestPainType
```

#### Classification Tree Model

We will start with a classification tree model:
```{r}
treeFit <- train(model, 
              data = train,
              method = "rpart",
              trControl = trainControl(method = "repeatedcv", 
                                       number = 10,
                                       repeats = 3),
              tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.001))
              )
```

##### Plot

This is a plot of our model:
```{r}
plot(treeFit)
```

#### Random Forest

Now we will create a Random Forest model:
```{r}
rfFit <- train(model, 
              data = train,
              method = "rf",
              trControl = trainControl(method = "repeatedcv", 
                                       number = 10,
                                       repeats = 3),
              tuneGrid = expand.grid(mtry = seq(from = 1, to = length(model1), by = 1))
              )
```

#### Boosted Tree

Now we will generate a Boosted Tree model:

```{r}
boostedFit <- train(model, 
              data = train,
              method = "gbm",
              trControl = trainControl(method = "repeatedcv", 
                                       number = 10,
                                       repeats = 3),
              tuneGrid = expand.grid(n.trees = c(25,50,100,200),
                                     interaction.depth = c(1,2,3),
                                     shrinkage = 0.1,
                                     n.minobsinnode = 10
                                     ),
              verbose = FALSE)
              
```

#### Comparison of the Three Models

We will generate the confusion matrix of all three models in order, and access the accuracy of each model:
```{r}
cat("Classification Tree Model", 
confusionMatrix(data = test$HeartDiseaseF,
                reference = predict(treeFit, newdata = test))$overall[1])
cat("Random Forest Model",
confusionMatrix(data = test$HeartDiseaseF,
                reference = predict(rfFit, newdata = test))$overall[1])
cat("Boosted Tree Model",
confusionMatrix(data = test$HeartDiseaseF,
                reference = predict(boostedFit, newdata = test))$overall[1])
```
### Final Analysis

As we can see, we nearly got 80% accuracy with our Boosted Model, giving it the highest accuracy of all the models we went through. This model overall did the best job on the test set.
